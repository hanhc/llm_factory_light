# Configuration for the inference API server
# Model path and TP size are primarily controlled by environment variables
# (MODEL_PATH, TP_SIZE) when using run_inference_api.sh or Docker.
# This file can hold other server or vLLM engine specific settings if needed.

server:
  host: "0.0.0.0"
  # port: 8000 # Port is better set via command-line arg or script

# vllm_engine: # If VLLMEngine were to take more config from here
  # tensor_parallel_size: 1 # Default, can be overridden by TP_SIZE env var
  # dtype: "auto"
  # max_model_len: null

logging:
  handlers:
    console:
      level: "INFO"
    file_handler:
      filename: "logs/inference_api.log"
      level: "INFO"
  loggers:
    llm_factory:
      level: "INFO"
    vllm: # vLLM's own logger
      level: "INFO"
    uvicorn:
      level: "INFO"