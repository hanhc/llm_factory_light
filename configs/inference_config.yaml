# configs/inference_config.yaml
# vLLM 通常从 MODEL_PATH 环境变量或启动参数获取模型，
# 但这里可以放一些 API 服务的其他配置，或者如果 vllm_engine.py 设计为从这里读
server:
  host: "0.0.0.0"
  # port: 8000 # 端口最好通过命令行参数或环境变量设置

# logging: # 日志配置可以沿用
#   ...