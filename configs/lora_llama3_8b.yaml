# 训练方法
training_method: lora

# 模型配置
model:
  name_or_path: "meta-llama/Meta-Llama-3-8B-Instruct"
  # quantization_config for QLoRA can be added here

# 数据配置
data:
  file_path: "./data/my_chat_dataset.jsonl"
  file_format: "jsonl"
  tokenizer_name_or_path: "meta-llama/Meta-Llama-3-8B-Instruct"

# LoRA 特定配置
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"

# 训练参数 (对应 transformers.TrainingArguments)
training_args:
  output_dir: "./output/llama3-8b-lora"
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-5
  logging_steps: 10
  save_steps: 100
  bf16: true # 如果你的 GPU 支持
  tf32: true # 如果你的 GPU 支持

# 日志配置 (可选，如果未提供，将使用 logging_setup.py 中的 DEFAULT_LOGGING_CONFIG)
# ... (model, data, lora, training_args sections from previous example) ...

# 日志配置 (可选，如果未提供，将使用 logging_setup.py 中的 DEFAULT_LOGGING_CONFIG)
logging:
  formatters:
    standard: # Override standard formatter if needed
      format: "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
  handlers:
    console:
      level: "INFO" # Set console level to INFO for this specific run
    file_handler:
      filename: "logs/my_experiment_llama3_lora.log" # Specific log file for this experiment
      level: "DEBUG"
  loggers:
    llm_factory:
      level: "DEBUG" # Get detailed logs from our application
      handlers: ["console", "file_handler"] # Ensure it uses our defined handlers
    transformers:
      level: "INFO" # Reduce verbosity from transformers for this run
  root:
    level: "INFO"

evaluation:
  model_to_evaluate_path: "./output/llama3-8b-lora" # Path to the fine-tuned model
  dataset_path: "wikitext" # Or path to a local file e.g., "./data/eval_data.txt"
  dataset_name: "wikitext-2-raw-v1" # If using HF datasets library identifier
  dataset_split: "test"
  text_column: "text" # Column in dataset containing text for perplexity
  metrics: ["perplexity"] # List of metrics to compute
  batch_size: 2 # Batch size for perplexity calculation
  perplexity_stride: 512
  output_file: "./output/llama3-8b-lora/eval_results.json"