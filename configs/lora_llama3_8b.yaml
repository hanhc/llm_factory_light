training_method: lora

model:
  name_or_path: "meta-llama/Meta-Llama-3-8B-Instruct" # Requires access
  # To test without access, use a smaller, open model like "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

data:
  file_path: "./data/sample_lora_dataset.jsonl" # Placeholder: Create this sample file
  file_format: "jsonl"
  tokenizer_name_or_path: "meta-llama/Meta-Llama-3-8B-Instruct" # Match model
  use_fast_tokenizer: true
  # DataProcessor should format this into prompt/response for LoRA template
  # Example template in DataProcessor: "### Human: {prompt}\n### Assistant: {response}"

# LoRA 特定配置
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    # - "gate_proj" # For Llama like models
    # - "up_proj"
    # - "down_proj"
  bias: "none"
  task_type: "CAUSAL_LM"


# 训练参数 (transformers.TrainingArguments)
training_args:
  output_dir: "./output/llama3-8b-lora"
  num_train_epochs: 1 # For quick test
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-5
  logging_steps: 10
  save_steps: 50
  save_total_limit: 2
  bf16: true
  # tf32: true
  report_to: "none"

# Evaluation config (optional, can be separate)
evaluation:
  model_to_evaluate_path: "./output/llama3-8b-lora" # After training
  dataset_path: "wikitext"
  dataset_name: "wikitext-2-raw-v1"
  dataset_split: "test"
  text_column: "text"
  metrics: ["perplexity"]
  batch_size: 1
  output_file: "./output/llama3-8b-lora/eval_results.json"

# 日志配置
logging:
  handlers:
    file_handler:
      filename: "logs/lora_llama3_8b_train.log"
  loggers:
    llm_factory:
      level: "DEBUG"
    transformers:
      level: "INFO"
    peft:
      level: "INFO"