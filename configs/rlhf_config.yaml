training_method: rlhf

rlhf:
  base_model_name: "./output/qwen1.5-7b-sft" # Path to your SFT-tuned model
  # reward_model_name: "path/to/your/reward_model_or_identifier" # Crucial! Using dummy in code.
  # For testing, you might need to use a public sentiment model or similar if you don't have a reward model
  reward_model_name: "distilbert-base-uncased-finetuned-sst-2-english" # Example public sentiment model

  ppo_config:
    batch_size: 2 # Small for testing
    mini_batch_size: 1
    gradient_accumulation_steps: 1
    learning_rate: 1.41e-6 # RL rates are usually smaller
    adap_kl_ctrl: true
    init_kl_coef: 0.02
    target: 6.0 # KL target
    horizon: 10000
    gamma: 1.0
    lam: 0.95
    cliprange: 0.2
    cliprange_value: 0.2
    vf_coef: 0.1
    ppo_epochs: 2 # PPO epochs per batch
    # log_with: "tensorboard" # or "wandb" or None

  tokenizer_name_or_path: "./output/qwen1.5-7b-sft" # Match SFT model
  generation_kwargs:
    max_new_tokens: 64
    # min_new_tokens: 32 # TRL PPO might not use min_new_tokens directly in generate call
    top_k: 0
    top_p: 1.0
    do_sample: true
    temperature: 0.9
    # pad_token_id will be set from tokenizer if None

data:
  prompt_dataset_path: "./data/sample_rlhf_prompts.jsonl" # Placeholder
  file_format: "jsonl"
  # DataProcessor's process_for_rl will be used
  max_prompt_length: 128 # Max length for tokenized prompts

training_args: # General control for the RL training script
  output_dir: "./output/my_model_rlhf_from_qwen_sft"
  total_ppo_epochs: 2 # Number of full passes over the prompt dataset
  save_freq: 1 # Save RL model every N PPO epochs

logging:
  handlers:
    file_handler:
      filename: "logs/my_model_rlhf.log"
  loggers:
    llm_factory:
      level: "INFO" # RL can be very verbose at DEBUG
    trl:
      level: "INFO"